
> # Here I will benchmark various different hyperparameters and architectures.
> 
> library(keras)

> FLAGS <- flags(
+   flag_integer("dense_units1", 8),
+   flag_numeric("dropout1", 0.3),
+   flag_string("activation1", 'relu'),
+   
+   flag_intege .... [TRUNCATED] 

> inputs <- layer_input(shape = c(dim(trainingData)[2]))

> output0 <- inputs %>%
+   layer_dropout(rate = FLAGS$dropout1) %>%
+   layer_dense(units = FLAGS$dense_units1, activation = eval(FLAGS$activation1)) .... [TRUNCATED] 

> makeOutputLayer <- function(x) {
+   layer_dense(output0, units=1, name= colnames(trainingLabels)[x])
+ }

> outputs <- lapply(1:dim(trainingLabels)[2], makeOutputLayer)

> model <- keras_model(inputs=inputs, outputs=outputs)

> model %>% compile(
+   loss = "mse",
+   optimizer = optimizer_rmsprop(lr = FLAGS$learning_rate),
+   metrics = list("mean_absolute_error"),
+   #lo .... [TRUNCATED] 

> #model %>% summary()
> 
> # Display training progress by printing a single dot for each completed epoch.
> print_dot_callback <- callback_lambda(
+  .... [TRUNCATED] 

> history <- model %>% fit(
+   x = trainingData,
+   y = lapply(1:dim(trainingLabels)[2], function(x) {trainingLabels[,x]}),
+   batch_size = FLAGS$b .... [TRUNCATED] 
