---
title: "DeepLearningSecond"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(BiocManager)
library(SingleCellExperiment)
library(purrr)
library(DropletUtils)
library(keras)
library(scran)
library(scater)
library(r10xDL)
library(umap)
```

Here we will be doing a quick foray into deep learning using the 10X genomics data published in June 2019. This data tracks around 150,000 cells across 4 different donors. For each cell, the data specify gene expression data, protein expression data, and antigen specificity data.

Broadly, our goal is to create a machine learning approach to predict protein expression from gene expression data. We have a lot of data here to work with, but it is not clear which machine learning approach will best be able to do this job. Questions include:

* Why is this useful?
* Has this been done before?
* Which type of basic architecture is best: Trees, Neural Nets, GANs?
* What kind of parameters do we want to consider (depending on architecture)?
* How transferable is the model from one individual to another individual?
* Can this be done in an accurate way?
* Can this be done in an efficient way?

For now, we will just proceed with a basic neural net.

### Cut up the data
```{r reading}
trainingSize = 10000 #how many cells to train on
testingSize = 500 #how many cells to test on
SCE <- read10xCounts("vignettes/10Xdata/vdj_v1_hs_aggregated_donor1_filtered_feature_bc_matrix.h5") ##55206 cells

geneTrainingSCE <- SCE[,1:trainingSize] #takes the first however many cells as training data
geneTestingSCE <- SCE[,(trainingSize+1):(trainingSize+testingSize)] #takes the next however many cells as testing data

proteinGREP <- "CD.*|Ig.*|HLA-DR"
proteinList <- grep(proteinGREP, rowData(geneTrainingSCE)$ID)

library(compositions)
proteinTrainingData <- geneTrainingSCE[proteinList] %>%
  counts() %>%
  as.matrix() %>%
  t() %>%
  clr()

trainingLabels <- proteinTrainingData[,colnames(proteinTrainingData) == "CD45RA"]

#testing
proteinTestingData <- geneTestingSCE[proteinList] %>%
  counts() %>%
  as.matrix() %>%
  t() %>%
  clr()
testingLabels <- proteinTestingData[,colnames(proteinTestingData) == "CD45RA"]

geneTrainingSCE <- geneTrainingSCE[rowData(geneTrainingSCE)$Type == "Gene Expression"]

testingLabels <- geneTestingSCE[rowData(geneTestingSCE)$ID == "CD45RA"] %>%
  counts() %>%
  as.matrix() %>%
  t()
geneTestingSCE <- geneTestingSCE[rowData(geneTestingSCE)$Type == "Gene Expression"]
```

```{r clean SCEs}
cellFilter <- function(x) {
  #Convert counts matrix
  counts(x) <- as.matrix(counts(x))
  x
  # x <- calculateQCMetrics(x, BPPARAM = BiocParallel::MulticoreParam())
  # 
  # #filter out low quality cells from the SCE
  # filt <- x$total_counts > 500 & x$total_features_by_counts > 100
  # x <- x[, filt]
}

library(scater)

geneTrainingSCE <- cellFilter(geneTrainingSCE)
geneTestingSCE <- cellFilter(geneTestingSCE)

#remove low frequency genes from the SCE (keep those with 1 read in at least 5% of cells)
min_reads <- 1
min_cells <- 0.05 * ncol(geneTrainingSCE) 
#calculate a boolean vector for which genes to keep
keep <- rowSums(counts(geneTrainingSCE) >= min_reads) >= min_cells
#subset both SCEs using this vector
geneTrainingSCE <- geneTrainingSCE[keep, ]
geneTestingSCE <- geneTestingSCE[keep, ]

#for readability, swap rownames from row ID to row symbol
#this may cause problems later
featureNames <- uniquifyFeatureNames(ID = rowData(geneTrainingSCE)$ID, names = rowData(geneTrainingSCE)$Symbol)
rownames(geneTrainingSCE) <- featureNames
rownames(geneTestingSCE) <- featureNames

#normalize
geneTrainingSCE <- normalize(geneTrainingSCE)
geneTestingSCE <- normalize(geneTestingSCE)
```

```{r featureSelect}
fit <- trendVar(geneTrainingSCE, use.spikes = FALSE)
dec <- decomposeVar(geneTrainingSCE, fit)
hvg <- dec$bio > 0 # save vector of genes
geneTrainingSCE <- geneTrainingSCE[hvg,]
geneTestingSCE <- geneTestingSCE[hvg,]
```

```{r separate}
trainingData <- SCEtoMatrix(geneTrainingSCE,transpose = TRUE)
testingData <- SCEtoMatrix(geneTestingSCE,transpose = TRUE)
```

```{r buildModel}
buildModel <- function() {
  model <- keras_model_sequential() %>%
    layer_dropout(rate = .3, input_shape = dim(trainingData)[2]) %>%
    layer_dense(units = 4, activation = "relu") %>%
    layer_dense(units = 4, activation = "relu") %>%
    layer_dense(units = 2, activation = "relu") %>%
    layer_dense(units = 1)

  model %>% compile(
    loss = "mse",
    optimizer = optimizer_rmsprop(),
    metrics = list("mean_absolute_error")
  )

  model
}

model <- buildModel()
model %>% summary()

# Display training progress by printing a single dot for each completed epoch.
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 80 == 0) cat("\n")
    cat(".")
  }
)    
epochs <- 80
batch_size <- 200

```

```{r trainModel, results = 'hide'}
# Fit the model and store training stats
history <- model %>% fit(
  trainingData,
  trainingLabels,
  batch_size = batch_size,
  epochs = epochs,
  validation_split = 0.2,
  verbose = 0,
  callbacks = list(print_dot_callback)
)

g <- plot(history, metrics = "mean_absolute_error", smooth = FALSE)
```

```{r graph, echo = FALSE}
g
qplot(testingLabels, predict(model, testingData))
cor(testingLabels, predict(model, testingData))
```



