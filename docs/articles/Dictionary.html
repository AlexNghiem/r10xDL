<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Dictionary • r10xDL</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Dictionary">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">r10xDL</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.0.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/DL%20Papers.html">DL Papers</a>
    </li>
    <li>
      <a href="../articles/Deep%20Learning%20Types.html">Deep Learning Types</a>
    </li>
    <li>
      <a href="../articles/DeepLearning.html">DeepLearning</a>
    </li>
    <li>
      <a href="../articles/Dictionary.html">Dictionary</a>
    </li>
    <li>
      <a href="../articles/Keras%20Guide.html">Keras Guide</a>
    </li>
    <li>
      <a href="../articles/TensorFlow%20example.html">TensorFlow example</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Dictionary</h1>
            
      
      
      <div class="hidden name"><code>Dictionary.Rmd</code></div>

    </div>

    
    
<div id="terms" class="section level1">
<h1 class="hasAnchor">
<a href="#terms" class="anchor"></a>Terms</h1>
<ul>
<li>hyperparameter - control aspect of training, rather than get trained (ie how fast the model learns)</li>
<li>dimensionality reduction - for a given data set with high-dimensional input, it projects the variable space onto a smaller space, while attempting to preserve most of the observed variance. Specifically, it attempts to keep the distances</li>
<li>autoencoder - a neural net that is trained to takes in an input set representing an object, reduce it through a bottleneck layer, and then recreate it on the other side</li>
<li>bottleneck layer - in an autoencoder, it is the smallest layer in the network (the least number of dimensions which the data is squeezed through).</li>
<li>p(y|x;<span class="math inline">\(\theta\)</span>) - this is the probability of y given a data set x and a set of hyperparameters <span class="math inline">\(\theta\)</span>
</li>
<li>log loss - <a href="https://www.youtube.com/watch?v=IYzc_2rY9k8">youtube video</a> a sum of error in a model. The calculation involves the averaging the negative log loss per sample from the data set, where the log loss in each sample is the natural log of the probability given by the model of the output occurring. For example, if my model predicts an 80% chance of it raining tomorrow, and it indeed rains tomorrow, then my model accrues from that sample a -ln(.8) in error. As the model becomes more confident in an answer, it will be punished less for getting it right and more for getting it wrong. To minimize log loss, our model must find a balance between confidence and caution.</li>
<li>max pooling - <a href="https://www.youtube.com/watch?v=ZjM_XQa5s6s">youtube video</a> basically it finds the maximum value in some block of the previous layer, and propagates only that value forwards, while preserving spatial relationships between blocks. This has the effect of vastly reducing the dimensionality of the network. Also possibly is average-value pooling, which obviously averages values in the pool. The blocks can overlap or not (usually not). The stride is how far the block moves each time, so if stride equals block size, there will be no overlap.</li>
<li>data manifold - the underlying biological data beneath the measurement noise. generally a model seeks to capture the nature of this data manifold</li>
<li>GAN (generative adversarial network) - 2014 framework for machine learning. Create TWO models, one generative (G) and one discriminative (D) and train them to compete against each other. G creates a fake sample, and D tries to ascertain the likelihood that the sample came from training data. Theoretically G should win and create such good fake samples that D cannot tell them apart from real samples. At the end both models are useful, G for generating realistic fake samples and D for telling whether a sample is part of a larger data set.</li>
<li>ZINB (Zero-Inflated Negative Binomial) - a negative binomial distribution with added zero inflation (perhaps due to some artificial effect like measurement failure in scRNA-seq data)</li>
<li>SVM (Support Vector Machine) - Translates data into higher-dimensional space, where it can potentially be clustered linearly (sounds like black magic to me).</li>
<li>Bias - a measure of how poorly a given TYPE OF MODEL can predict the true data relationship. For example, a linear model will always be biased against exponential data</li>
<li>Variance - for a given TYPE OF MODEL, the variance is a measure of how variant the sums of squares are for different data sets. For example, a hyper-overfitted model will have low bias (given that the training data follows the true data relationship), but high variance. Critically, as a type of model gets closer to the true data relationship (as bias decreases), its variance should also go to 0, as most data sets will have similar distributions around that true relationship.</li>
<li>BAGGing (Bootstrap AGGregatING) - samples a small population from a larger population, and trains a model on that subset (this is bootstrapping). then aggregates these models when discriminating new samples (aggregating).</li>
<li>Boosting - iteratively follows this pattern: trains a weak learner and then re-weights the samples.</li>
</ul>
</div>
<div id="tools" class="section level1">
<h1 class="hasAnchor">
<a href="#tools" class="anchor"></a>Tools</h1>
<ul>
<li>scVI (Single Cell Variational Inference) - 2018 tool for scRNA-seq data analysis. Can perform dim red, visualization, clustering, normalization, and lots of error reductions. uses deep learning to clean data. beats out methods such as MAGIC and DCA, might lose to GAN</li>
<li>PCA (Principal Components Analysis) - linearly transforms an input space into a reduced dimension space. Notably, this is a form of linear autoencoder</li>
<li>t-SNE (t-distributed stochastic neighbor embedding) - a strange method of dimensional reduction that projects data in high dimensions onto a lower dimensional space, and then slowly adjusts the data points until the original distances are preserved. Notably, this process is only useful for visualizing. The dimensions don’t have semantic meaning anymore, as all of the data points get shifted simply to preserve distance.</li>
<li>DCA (Deep Count Autoencoder) - a method using DL to denoise gex data. Scales linearly for large data, so very tractable</li>
<li>scImpute - imputation method which first calculates likely dropout values and then subsequently imputes only those selected values. (it is not clear to me how this is a bad thing)</li>
<li>Splatter - a method to simulate scRNA-seq data, with or without dropout</li>
<li>Leave one out - A method of testing optimal parameters: with n samples, we train on n-1 of them and test on the remaining one. Then we repeat this process n times, each time leaving a different sample out and training on it.</li>
<li>confusion matrix - the matrix of actual positives vs negatives, and predicted positives vs negatives. This tabulates true positives, false positives, true negatives, and false negatives.</li>
</ul>
</div>
<div id="unknown" class="section level1">
<h1 class="hasAnchor">
<a href="#unknown" class="anchor"></a>Unknown</h1>
<ul>
<li>MAGIC (Markov Affinity-based Graph Imputation of Cells) -</li>
<li>SAVER (Single-cell Analysis Via Expression Recovery) -</li>
<li>k-NN classification</li>
<li>Gradient Boosted Tree</li>
</ul>
</div>
<div id="to-investigate" class="section level1">
<h1 class="hasAnchor">
<a href="#to-investigate" class="anchor"></a>To investigate</h1>
<ul>
<li>Ridge Regression - standard linear regression optimized with sum-of-squares error, but with a twist. you add an error term to the classic sum of squared error, which is (<span class="math inline">\(\lambda\)</span> * [slope^2]). Lambda is a positive coefficient determined by trial and error, and slope^2 is actually the sum of the squared slopes for the coefficients of each term in the linear regression. For example, the equation y = a + bx + cy, then the error is [standard sum of squares error] + <span class="math inline">\(\lambda\)</span>(b^2 + c^2). This has the effect of making it safer for the algorithm to assume less of a relationship between the independent and dependent variable. For example, when predicting weight of a person from height, we want to guess a slightly lower upward slope than the data suggest just to be safe. this helps account for the fact that we are more likely to overestimate the slope than underestimate it.</li>
<li>Lasso Regression - similar to ridge regression, but instead of squaring each slope term, we take the absolute value of it. The key difference betweeen ridge regression and lasso regression is that the error term in lasso regression approaches 0 much slower as the slope approaches 0. Therefore, lasso will sometimes incentivize completely zeroing out slopes, whereas ridge regression will almost always suggest at least some very small slope, as this marginally reduces residuals without really affecting the new error term.</li>
<li>Elastic Net Regression - Really just a combination of ridge and lasso: for arbitrary non-negative values <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>, we calculate error using error = standard sum of error + <span class="math inline">\(\lambda_1\)</span> * ([slope^2]) + <span class="math inline">\(\lambda_2\)</span> * (|slope|). Setting different lambdas to 0 reduces this to either type of regression. Somehow the combination of effects makes it better at dealing with correlated parameters.</li>
<li>Decision Tree -</li>
<li>Random Forest -</li>
<li>Boosted Forest -</li>
<li>Gradient Boosted Decision Forest -</li>
</ul>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#terms">Terms</a></li>
      <li><a href="#tools">Tools</a></li>
      <li><a href="#unknown">Unknown</a></li>
      <li><a href="#to-investigate">To investigate</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Alex Nghiem.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.3.0.</p>
</div>
      </footer>
</div>

  

  </body>
</html>
